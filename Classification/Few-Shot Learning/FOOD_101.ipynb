{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.13"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "r6NgmaME_1rK",
        "Tzxt5nDla2bc",
        "Bzhcax0yEREe",
        "iD-zp0QPi7es",
        "NoXVD9GWhyoL"
      ],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tzxt5nDla2bc"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo1H0lVFjOQ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8232377c-5816-44cb-e100-83986b64c66c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsqmGk_kaf6s"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.enable_eager_execution()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_ngCHVwZ8qi"
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop, SGD, Adam,Adamax\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.keras.losses import Loss\n",
        "from tensorflow.python.keras.utils import losses_utils\n",
        "from tensorflow.python.keras.utils import tf_utils\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.python.ops import gen_math_ops\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, ZeroPadding2D,Dropout, Dense, Input, concatenate,GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import  AveragePooling2D, Flatten,ZeroPadding2D,AveragePooling1D,Activation,BatchNormalization\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.optimizers import RMSprop, SGD, Adam,Adadelta,Adamax\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau,LearningRateScheduler\n",
        "from keras.models import load_model\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cdR2sjRW4kx"
      },
      "source": [
        "# DATA *GENERATOR*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmI7dgf3Gr0n"
      },
      "outputs": [],
      "source": [
        "#https://github.com/sayakpaul/ViT-jax2tf/blob/main/fine_tune.ipynb\n",
        "BATCH_SIZE = 32\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "num_classes = 101\n",
        "\n",
        "def make_dataset(dataset: tf.data.Dataset, train: bool, image_size: int = 224):\n",
        "    def preprocess(image, label):\n",
        "        # For training, do a random crop and horizontal flip.\n",
        "        if train:\n",
        "            channels = image.shape[-1]\n",
        "            begin, size, _ = tf.image.sample_distorted_bounding_box(\n",
        "                tf.shape(image),\n",
        "                tf.zeros([0, 0, 4], tf.float32),\n",
        "                area_range=(0.05, 1.0),\n",
        "                min_object_covered=0,\n",
        "                use_image_if_no_bounding_boxes=True,\n",
        "            )\n",
        "            image = tf.slice(image, begin, size)\n",
        "\n",
        "            image.set_shape([None, None, channels])\n",
        "            image = tf.image.resize(image, [image_size, image_size])\n",
        "            if tf.random.uniform(shape=[]) > 0.5:\n",
        "                image = tf.image.flip_left_right(image)\n",
        "\n",
        "        else:\n",
        "            image = tf.image.resize(image, [image_size, image_size])\n",
        "\n",
        "        image = (image - 127.5) / 127.5\n",
        "        return image, label\n",
        "\n",
        "    if train:\n",
        "        dataset = dataset.shuffle(BATCH_SIZE * 10)\n",
        "    return dataset.map(preprocess, AUTO).batch(BATCH_SIZE).prefetch(AUTO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1539e2ba-58f5-4df4-86ed-e8c527ff6a8a",
        "id": "y7L3LKW6DRDn"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 75750\n",
            "Number of validation examples: 25250\n"
          ]
        }
      ],
      "source": [
        "#https://github.com/KlrShaK/Oxford_flowers102-using-Tensorflow/blob/master/Classifier/oxford_flower102.py\n",
        "dataset_name = 'food101'\n",
        "train_dataset, val_dataset = tfds.load(dataset_name, split=['train', 'validation'], as_supervised=True)\n",
        "num_train = tf.data.experimental.cardinality(train_dataset)\n",
        "num_val = tf.data.experimental.cardinality(val_dataset)\n",
        "print(f\"Number of training examples: {num_train}\")\n",
        "print(f\"Number of validation examples: {num_val}\")\n",
        "train_dataset = make_dataset(train_dataset, True)\n",
        "val_dataset = make_dataset(val_dataset, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIWGAWUhV1QH"
      },
      "outputs": [],
      "source": [
        "b1_vit=\"https://tfhub.dev/sayakpaul/vit_b8_classification/1\"\n",
        "feature_extractor_model = b1_vit\n",
        "feature_extractor_layer = hub.KerasLayer(feature_extractor_model,input_shape=(224,224 ,3),trainable=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD-zp0QPi7es"
      },
      "source": [
        "# Model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l2=tf.keras.regularizers.l2(1e-4)\n",
        "l1l2=regularizers.l1_l2(l1=1e-5, l2=1e-4)\n",
        "l1=tf.keras.regularizers.l1(1e-5)\n",
        "kernel_init = tf.keras.initializers.he_normal()\n",
        "l2=regularizers.l2(0.01)\n",
        "bias_init = tf.keras.initializers.Constant(value=0.1)"
      ],
      "metadata": {
        "id": "rIbTZK8roga7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FYOdngzf2bH"
      },
      "source": [
        "def inception_module1(x,\n",
        "                     filters_3x3,\n",
        "                     filters_3x3_1,\n",
        "                     filters_3x3_2,\n",
        "                     filters_5x5,\n",
        "                     filters_5x5_1,\n",
        "                     filters_5x5_2,\n",
        "                     filters_pool_proj,\n",
        "                     name='inc1'):\n",
        "\n",
        "    conv_3x3 = Conv2D(filters_3x3, (1, 1), padding='same', activation='elu', kernel_initializer=\"he_normal\",trainable=True)(x)\n",
        "    conv_3x3_1 = Conv2D(filters_3x3_1, (3, 3), padding='same', activation='elu', kernel_initializer=\"he_normal\",trainable=True)(conv_3x3)\n",
        "    conv_3x3_2 = Conv2D(filters_3x3_2, (3, 3), padding='same', activation='elu', kernel_initializer=\"he_normal\",kernel_regularizer=l2,trainable=True)(conv_3x3)\n",
        "\n",
        "    conv_5x5 = Conv2D(filters_5x5, (1, 1), padding='same', activation='elu', kernel_initializer=kernel_init,trainable=True)(x)\n",
        "    conv_5x5_1= Conv2D(filters_5x5_1, (5, 5), padding='same', activation='elu', kernel_initializer=\"he_normal\",trainable=True)(conv_5x5)\n",
        "    conv_5x5_2 = Conv2D(filters_5x5_2, (5, 5), padding='same', activation='elu', kernel_initializer=\"he_normal\",kernel_regularizer=l2,trainable=True)(conv_5x5)\n",
        "\n",
        "    pool_proj = MaxPool2D((3, 3), strides=(1, 1), padding='same')(x)\n",
        "    pool_proj = Conv2D(filters_pool_proj, (1, 1), padding='same', activation='elu', kernel_initializer=\"he_normal\")(pool_proj)\n",
        "\n",
        "    output = concatenate([conv_3x3_1, conv_3x3_2, conv_5x5_1,conv_5x5_2, pool_proj], axis=3, name='inc1')\n",
        "\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1meRFxyP7gfa"
      },
      "source": [
        "def inception_module2(x,\n",
        "                     filters_3x3,\n",
        "                     filters_3x3_1,\n",
        "                     filters_3x3_2,\n",
        "                     filters_5x5,\n",
        "                     filters_5x5_1,\n",
        "                     filters_5x5_2,\n",
        "                     filters_pool_proj,\n",
        "                     name='inc21'):\n",
        "\n",
        "    conv_3x3 = Conv2D(filters_3x3, (1, 1), padding='same', activation='elu', kernel_initializer=\"he_normal\",trainable=True)(x)\n",
        "    conv_3x3_1 = Conv2D(filters_3x3_1, (3, 3), padding='same', activation='elu', kernel_initializer=\"he_normal\",trainable=True)(conv_3x3)\n",
        "    conv_3x3_2 = Conv2D(filters_3x3_2, (3, 3), padding='same', activation='elu', kernel_initializer=\"he_normal\",kernel_regularizer=l2,trainable=True)(conv_3x3)\n",
        "\n",
        "    conv_5x5 = Conv2D(filters_5x5, (1, 1), padding='same', activation='elu', kernel_initializer=kernel_init,trainable=True)(x)\n",
        "    conv_5x5_1= Conv2D(filters_5x5_1, (5, 5), padding='same', activation='elu', kernel_initializer=\"he_normal\",trainable=True)(conv_5x5)\n",
        "    conv_5x5_2 = Conv2D(filters_5x5_2, (5, 5), padding='same', activation='elu', kernel_initializer=\"he_normal\",kernel_regularizer=l2,trainable=True)(conv_5x5)\n",
        "\n",
        "    pool_proj = MaxPool2D((3, 3), strides=(1, 1), padding='same')(x)\n",
        "    pool_proj = Conv2D(filters_pool_proj, (1, 1), padding='same', activation='elu', kernel_initializer=\"he_normal\")(pool_proj)\n",
        "\n",
        "    output = concatenate([conv_3x3_1, conv_3x3_2, conv_5x5_1,conv_5x5_2, pool_proj], axis=3, name='inc21')\n",
        "\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwkMRhM0fJ_l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d694c5ec-f5d1-4c84-934f-60cb09f32384"
      },
      "source": [
        "model = Sequential()\n",
        "input_layer = Input(shape=(224,224,3))\n",
        "\n",
        "vit =   feature_extractor_layer(input_layer)\n",
        "x9 = Dense(num_classes, activation='softmax',name='vit_')(vit)\n",
        "\n",
        "x_01=Conv2D(32, (3, 3), padding = 'same', kernel_initializer=\"he_normal\",name='conv2d_10_')(input_layer)\n",
        "x2=Activation('elu')(x_01)\n",
        "x3=BatchNormalization(name='batch_10')(x2)\n",
        "\n",
        "x4=Conv2D(64, (1,1), padding = \"same\", kernel_initializer=\"he_normal\",name='conv1d_1')(x3)\n",
        "\n",
        "x5=Conv2D(64, (3,3), padding = \"same\", kernel_initializer=\"he_normal\",name='conv2d_102')(x4)\n",
        "x6=Activation('elu')(x5)\n",
        "x7=BatchNormalization()(x6)\n",
        "x8=Dropout(0.5)(x7)\n",
        "\n",
        "x9s=Conv2D(64, (3,3), padding = \"same\", kernel_initializer=\"he_normal\",name='conv2d_103')(x8)\n",
        "x0=Activation('elu')(x9s)\n",
        "xy=BatchNormalization()(x0)\n",
        "\n",
        "x0=AveragePooling2D(pool_size=(2, 2))(xy)\n",
        "\n",
        "y1=AveragePooling2D(pool_size=(4, 4))(xy)\n",
        "y1=Dropout(0.2)(y1)\n",
        "\n",
        "x1=x0\n",
        "##############################################\n",
        "i0=Conv2D(64, (1,1), padding = \"same\", kernel_initializer=\"he_normal\")(x1)\n",
        "\n",
        "i1 = inception_module1(i0,\n",
        "                     filters_3x3=128,\n",
        "                     filters_3x3_1=192,\n",
        "                     filters_3x3_2=192,\n",
        "                     filters_5x5=64,\n",
        "                     filters_5x5_1=96,\n",
        "                     filters_5x5_2=96,\n",
        "                     filters_pool_proj=64,\n",
        "                     name='inc1')\n",
        "\n",
        "i2 = inception_module2(i1,\n",
        "                     filters_3x3=128,\n",
        "                     filters_3x3_1=192,\n",
        "                     filters_3x3_2=192,\n",
        "                     filters_5x5=64,\n",
        "                     filters_5x5_1=96,\n",
        "                     filters_5x5_2=96,\n",
        "                     filters_pool_proj=64,\n",
        "                     name='inc2')\n",
        "\n",
        "i4 = AveragePooling2D(pool_size=(6,6), padding='same', strides=None, name='max_pool_inception_01')(i1)\n",
        "##############################################\n",
        "x1z=Conv2D(128, (3, 3), padding = 'same', kernel_initializer=\"he_normal\",name='conv2d_201')(x1)\n",
        "x1x=Activation('elu')(x1z)\n",
        "x1c=BatchNormalization(name='batch_12')(x1x)\n",
        "\n",
        "x1v=Conv2D(128, (1,1), padding = \"same\", kernel_initializer=\"he_normal\",name='conv1d_2')(x1c)\n",
        "\n",
        "x13=ZeroPadding2D((1, 1))(x1v)\n",
        "x12=Conv2D(128, (3, 3), padding = 'same', kernel_initializer=\"he_normal\",name='conv2d_202')(x13)\n",
        "x18=Activation('elu')(x12)\n",
        "x19=BatchNormalization(name='batch_128')(x18)\n",
        "x126=Conv2D(128, (3, 3), padding = 'valid', kernel_initializer=\"he_normal\",name='conv203')(x19)\n",
        "x1e=Activation('elu')(x126)\n",
        "x23r=BatchNormalization()(x1e)\n",
        "x23rr=ZeroPadding2D((2,2))(x23r)\n",
        "x23t=Dropout(0.2)   (x23rr)\n",
        "x23= MaxPooling2D(pool_size=(6,6))(x23t)\n",
        "x21= AveragePooling2D(pool_size=(2, 2))(x23r)\n",
        "\n",
        "x2t=concatenate([y1,x21])\n",
        "\n",
        "x3r=Conv2D(256, (3, 3), padding = 'same', kernel_initializer=\"he_normal\",name='conv2d_3_1')(x2t)\n",
        "x3e=Activation('elu')(x3r)\n",
        "x35w=BatchNormalization()(x3e)\n",
        "x34q=Conv2D(256, (1,1), padding = \"same\", kernel_initializer=\"he_normal\",name='conv1d_301')(x35w)\n",
        "x36j=Conv2D(256, (3, 3), padding = 'same', kernel_initializer=\"he_normal\",name='conv2d_302')(x34q)\n",
        "x36h=Activation('elu')(x36j)\n",
        "x3166g=BatchNormalization()(x36h)\n",
        "x3166f=Dropout(0.2)(x3166g)\n",
        "x316=ZeroPadding2D((1,1))(x3166f)\n",
        "x33= MaxPooling2D(pool_size=(3,3))(x316)\n",
        "x3_1=AveragePooling2D(pool_size=(2, 2)) (x3166f)\n",
        "x41=ZeroPadding2D((1, 1))(x3_1)\n",
        "x42=Conv2D(512, (3, 3), padding = 'same', kernel_initializer=\"he_normal\",name='conv2d_401_')(x41)\n",
        "x43=Activation('elu')(x42)\n",
        "x44=BatchNormalization(name='batch521')(x43)\n",
        "x65=Conv2D(512, (1,1), padding = \"same\", kernel_initializer=\"he_normal\",name='conv1d_4_')(x44)\n",
        "x66=Conv2D(512, (3, 3), padding = 'valid', kernel_initializer=\"he_normal\",name='conv2d_402')(x65)\n",
        "x67=Activation('elu')(x66)\n",
        "x68=BatchNormalization()(x67)\n",
        "x7 =AveragePooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(x68)\n",
        "dc_1_0 = Conv2DTranspose(512, (6,6), activation='elu',strides=1,\n",
        "      padding = 'valid',kernel_initializer='he_normal', name='deconv_01_')(x7)\n",
        "dc_1_0=concatenate([i4,dc_1_0,x23,x33])\n",
        "dc_1 = UpSampling2D((2,2))(dc_1_0)\n",
        "x7_0 = Flatten()(dc_1_0)\n",
        "x7_1 = Dense(256, activation='elu',name='dense1')(x7_0)\n",
        "x7_22 =BatchNormalization()(x7_1)\n",
        "x7_2 = Dropout(0.1)(x7_22)\n",
        "x7_4 = Dense(128, activation='elu',name='dense2')(x7_2)\n",
        "x7_51 =BatchNormalization()(x7_4)\n",
        "x7_5 = Dropout(0.1)(x7_51)\n",
        "x8w = Dense(num_classes, activation='softmax',name='dense')(x7_5)\n",
        "avr= tf.keras.layers.Average(name='average')([x8w,x9])\n",
        "#########################\n",
        "model = Model(input_layer, [x8w,x9,avr], name='Combination_Model_01')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer HeNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XawOcnfi7et"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tensorboard callback (functionized because need to create a new one for each model)\n",
        "import datetime\n",
        "def create_tensorboard_callback(dir_name, experiment_name):\n",
        "  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "      log_dir=log_dir\n",
        "  )\n",
        "  print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
        "  return tensorboard_callback\n",
        "\n",
        "#############################\n",
        "\n",
        "  # Setup EarlyStopping callback to stop training if model's val_loss doesn't improve for 3 epochs\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", # watch the val loss metric\n",
        "                                                  patience=10) # if val loss decreases for 3 epochs in a row, stop training\n",
        "##############################\n",
        "# Creating learning rate reduction callback\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
        "                                                 factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x)\n",
        "                                                 patience=2,\n",
        "                                                 verbose=1, # print out when learning rate goes down\n",
        "                                                 min_lr=1e-7)\n",
        "###############################\n",
        "def decay(epochs, steps=100):\n",
        "    initial_lrate = 0.0001\n",
        "    drop = 0.96\n",
        "    epochs_drop = 8\n",
        "    lrate = initial_lrate * math.pow(drop, math.floor((1+(2*epochs)/epochs_drop)))\n",
        "    return lrate\n",
        "################################\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"/flower_1.h5\",\n",
        "                             #save the best model while all epovhs of training\n",
        "                             monitor=\"val_loss\",\n",
        "                             save_best_only=True, # only save the best weights\n",
        "                             save_weights_only=True, # only save model weights (not whole model)\n",
        "                             mode=\"min\",\n",
        "                             verbose=1)\n",
        "\n",
        "#################################\n",
        "lr_sc = LearningRateScheduler(decay,verbose=1)\n",
        "mycallback = [early_stopping,lr_sc,reduce_lr,checkpoint,\n",
        "              create_tensorboard_callback(dir_name=\"tensorflow_hub\",\n",
        "              experiment_name=\"ENSEMBLE MODEL CIFAR10_Exp2\")]"
      ],
      "metadata": {
        "id": "ccNIYxWEqR7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model('/ensemble_food101_model_1')"
      ],
      "metadata": {
        "id": "aPhze9n5fuLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_train_samples = 75750\n",
        "nb_validation_samples = 25250\n",
        "\n",
        "lossWeights = {'dense3':0.25,'vit_':0.75}\n",
        "loss5 = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "optimizer =Adamax(learning_rate=0.002, clipnorm=1.0)#optimizer =ad\n",
        "\n",
        "model.compile( optimizer=optimizer, metrics=[\"accuracy\"],loss={'dense3':loss5,'vit_':loss5},loss_weights=lossWeights)\n",
        "\n",
        "history = model.fit(train_dataset\n",
        "                    ,validation_data=val_dataset\n",
        "                    ,epochs=100\n",
        "                    ,callbacks=[mycallback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxbbZVFPgxH4",
        "outputId": "a1e2a9c7-2494-45fc-e069-e1dcb05c8221"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 9.6e-05.\n",
            "Epoch 1/100\n",
            "4735/4735 [==============================] - 2650s 557ms/step - loss: 2.9991 - dense3_loss: 4.3490 - vit__loss: 2.4869 - dense3_accuracy: 0.0623 - vit__accuracy: 0.4388 - average_accuracy: 0.4370 - val_loss: 1.7639 - val_dense3_loss: 4.0951 - val_vit__loss: 0.9400 - val_dense3_accuracy: 0.1048 - val_vit__accuracy: 0.7847 - val_average_accuracy: 0.7814 - lr: 9.6000e-05\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 9.6e-05.\n",
            "Epoch 2/100\n",
            "4735/4735 [==============================] - 2628s 555ms/step - loss: 2.1531 - dense3_loss: 4.0052 - vit__loss: 1.4985 - dense3_accuracy: 0.1088 - vit__accuracy: 0.6320 - average_accuracy: 0.6303 - val_loss: 1.4274 - val_dense3_loss: 3.6320 - val_vit__loss: 0.6637 - val_dense3_accuracy: 0.1683 - val_vit__accuracy: 0.8365 - val_average_accuracy: 0.8332 - lr: 9.6000e-05\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 9.6e-05.\n",
            "Epoch 3/100\n",
            "4735/4735 [==============================] - 2627s 555ms/step - loss: 1.9291 - dense3_loss: 3.7632 - vit__loss: 1.2947 - dense3_accuracy: 0.1488 - vit__accuracy: 0.6753 - average_accuracy: 0.6733 - val_loss: 1.3107 - val_dense3_loss: 3.4732 - val_vit__loss: 0.5713 - val_dense3_accuracy: 0.1909 - val_vit__accuracy: 0.8535 - val_average_accuracy: 0.8489 - lr: 9.6000e-05\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 9.6e-05.\n",
            "Epoch 4/100\n",
            "4735/4735 [==============================] - 2626s 554ms/step - loss: 1.8023 - dense3_loss: 3.5662 - vit__loss: 1.1986 - dense3_accuracy: 0.1839 - vit__accuracy: 0.6955 - average_accuracy: 0.6935 - val_loss: 1.1844 - val_dense3_loss: 3.1333 - val_vit__loss: 0.5215 - val_dense3_accuracy: 0.2630 - val_vit__accuracy: 0.8614 - val_average_accuracy: 0.8582 - lr: 9.6000e-05\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 9.216e-05.\n",
            "Epoch 5/100\n",
            "4735/4735 [==============================] - 2627s 555ms/step - loss: 1.7050 - dense3_loss: 3.3820 - vit__loss: 1.1347 - dense3_accuracy: 0.2193 - vit__accuracy: 0.7130 - average_accuracy: 0.7095 - val_loss: 1.1072 - val_dense3_loss: 2.9368 - val_vit__loss: 0.4877 - val_dense3_accuracy: 0.2981 - val_vit__accuracy: 0.8690 - val_average_accuracy: 0.8670 - lr: 9.2160e-05\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 9.216e-05.\n",
            "Epoch 6/100\n",
            "4735/4735 [==============================] - 2627s 555ms/step - loss: 1.6343 - dense3_loss: 3.2260 - vit__loss: 1.0957 - dense3_accuracy: 0.2525 - vit__accuracy: 0.7204 - average_accuracy: 0.7174 - val_loss: 1.0612 - val_dense3_loss: 2.8260 - val_vit__loss: 0.4659 - val_dense3_accuracy: 0.3248 - val_vit__accuracy: 0.8728 - val_average_accuracy: 0.8686 - lr: 9.2160e-05\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 9.216e-05.\n",
            "Epoch 7/100\n",
            "4735/4735 [==============================] - 2630s 555ms/step - loss: 1.5640 - dense3_loss: 3.0610 - vit__loss: 1.0590 - dense3_accuracy: 0.2843 - vit__accuracy: 0.7299 - average_accuracy: 0.7263 - val_loss: 1.0011 - val_dense3_loss: 2.6509 - val_vit__loss: 0.4461 - val_dense3_accuracy: 0.3530 - val_vit__accuracy: 0.8791 - val_average_accuracy: 0.8716 - lr: 9.2160e-05\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 9.216e-05.\n",
            "Epoch 8/100\n",
            "4735/4735 [==============================] - 2633s 556ms/step - loss: 1.5195 - dense3_loss: 2.9403 - vit__loss: 1.0415 - dense3_accuracy: 0.3092 - vit__accuracy: 0.7327 - average_accuracy: 0.7292 - val_loss: 0.9146 - val_dense3_loss: 2.3471 - val_vit__loss: 0.4334 - val_dense3_accuracy: 0.4228 - val_vit__accuracy: 0.8810 - val_average_accuracy: 0.8750 - lr: 9.2160e-05\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 8.847359999999999e-05.\n",
            "Epoch 9/100\n",
            "4735/4735 [==============================] - 2629s 555ms/step - loss: 1.4557 - dense3_loss: 2.8057 - vit__loss: 1.0023 - dense3_accuracy: 0.3385 - vit__accuracy: 0.7425 - average_accuracy: 0.7372 - val_loss: 0.8620 - val_dense3_loss: 2.1899 - val_vit__loss: 0.4164 - val_dense3_accuracy: 0.4507 - val_vit__accuracy: 0.8872 - val_average_accuracy: 0.8791 - lr: 8.8474e-05\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 8.847359999999999e-05.\n",
            "Epoch 10/100\n",
            "4735/4735 [==============================] - 2629s 555ms/step - loss: 1.4250 - dense3_loss: 2.7132 - vit__loss: 0.9929 - dense3_accuracy: 0.3600 - vit__accuracy: 0.7453 - average_accuracy: 0.7411 - val_loss: 0.8439 - val_dense3_loss: 2.1470 - val_vit__loss: 0.4072 - val_dense3_accuracy: 0.4650 - val_vit__accuracy: 0.8889 - val_average_accuracy: 0.8827 - lr: 8.8474e-05\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 8.847359999999999e-05.\n",
            "Epoch 11/100\n",
            "4735/4735 [==============================] - 2626s 555ms/step - loss: 1.3887 - dense3_loss: 2.6210 - vit__loss: 0.9759 - dense3_accuracy: 0.3783 - vit__accuracy: 0.7489 - average_accuracy: 0.7450 - val_loss: 0.8050 - val_dense3_loss: 2.0128 - val_vit__loss: 0.4006 - val_dense3_accuracy: 0.4910 - val_vit__accuracy: 0.8909 - val_average_accuracy: 0.8827 - lr: 8.8474e-05\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 8.847359999999999e-05.\n",
            "Epoch 12/100\n",
            "4735/4735 [==============================] - 2631s 556ms/step - loss: 1.3640 - dense3_loss: 2.5469 - vit__loss: 0.9681 - dense3_accuracy: 0.3943 - vit__accuracy: 0.7500 - average_accuracy: 0.7453 - val_loss: 0.7835 - val_dense3_loss: 1.9483 - val_vit__loss: 0.3938 - val_dense3_accuracy: 0.5094 - val_vit__accuracy: 0.8931 - val_average_accuracy: 0.8853 - lr: 8.8474e-05\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 8.493465599999999e-05.\n",
            "Epoch 13/100\n",
            "4735/4735 [==============================] - 2630s 555ms/step - loss: 1.3325 - dense3_loss: 2.4706 - vit__loss: 0.9520 - dense3_accuracy: 0.4100 - vit__accuracy: 0.7551 - average_accuracy: 0.7505 - val_loss: 0.7482 - val_dense3_loss: 1.8269 - val_vit__loss: 0.3875 - val_dense3_accuracy: 0.5366 - val_vit__accuracy: 0.8941 - val_average_accuracy: 0.8858 - lr: 8.4935e-05\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 8.493465599999999e-05.\n",
            "Epoch 14/100\n",
            "4735/4735 [==============================] - 2630s 555ms/step - loss: 1.3087 - dense3_loss: 2.3937 - vit__loss: 0.9460 - dense3_accuracy: 0.4272 - vit__accuracy: 0.7559 - average_accuracy: 0.7509 - val_loss: 0.7273 - val_dense3_loss: 1.7655 - val_vit__loss: 0.3804 - val_dense3_accuracy: 0.5520 - val_vit__accuracy: 0.8964 - val_average_accuracy: 0.8872 - lr: 8.4935e-05\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 8.493465599999999e-05.\n",
            "Epoch 15/100\n",
            "4735/4735 [==============================] - 2630s 555ms/step - loss: 1.2862 - dense3_loss: 2.3412 - vit__loss: 0.9338 - dense3_accuracy: 0.4385 - vit__accuracy: 0.7587 - average_accuracy: 0.7548 - val_loss: 0.7248 - val_dense3_loss: 1.7603 - val_vit__loss: 0.3790 - val_dense3_accuracy: 0.5613 - val_vit__accuracy: 0.8958 - val_average_accuracy: 0.8873 - lr: 8.4935e-05\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 8.493465599999999e-05.\n",
            "Epoch 16/100\n",
            "4735/4735 [==============================] - 2626s 555ms/step - loss: 1.2676 - dense3_loss: 2.2849 - vit__loss: 0.9279 - dense3_accuracy: 0.4516 - vit__accuracy: 0.7605 - average_accuracy: 0.7556 - val_loss: 0.7003 - val_dense3_loss: 1.6822 - val_vit__loss: 0.3724 - val_dense3_accuracy: 0.5706 - val_vit__accuracy: 0.8979 - val_average_accuracy: 0.8899 - lr: 8.4935e-05\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 8.153726975999998e-05.\n",
            "Epoch 17/100\n",
            "4735/4735 [==============================] - 2626s 555ms/step - loss: 1.2424 - dense3_loss: 2.2287 - vit__loss: 0.9132 - dense3_accuracy: 0.4645 - vit__accuracy: 0.7641 - average_accuracy: 0.7607 - val_loss: 0.6913 - val_dense3_loss: 1.6533 - val_vit__loss: 0.3702 - val_dense3_accuracy: 0.5763 - val_vit__accuracy: 0.8976 - val_average_accuracy: 0.8888 - lr: 8.1537e-05\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 8.153726975999998e-05.\n",
            "Epoch 18/100\n",
            "4735/4735 [==============================] - 2625s 554ms/step - loss: 1.2343 - dense3_loss: 2.1899 - vit__loss: 0.9153 - dense3_accuracy: 0.4727 - vit__accuracy: 0.7628 - average_accuracy: 0.7597 - val_loss: 0.6606 - val_dense3_loss: 1.5431 - val_vit__loss: 0.3661 - val_dense3_accuracy: 0.6034 - val_vit__accuracy: 0.8994 - val_average_accuracy: 0.8902 - lr: 8.1537e-05\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 8.153726975999998e-05.\n",
            "Epoch 19/100\n",
            "4735/4735 [==============================] - 2625s 554ms/step - loss: 1.2078 - dense3_loss: 2.1429 - vit__loss: 0.8957 - dense3_accuracy: 0.4809 - vit__accuracy: 0.7685 - average_accuracy: 0.7655 - val_loss: 0.6406 - val_dense3_loss: 1.4724 - val_vit__loss: 0.3630 - val_dense3_accuracy: 0.6183 - val_vit__accuracy: 0.9004 - val_average_accuracy: 0.8912 - lr: 8.1537e-05\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 8.153726975999998e-05.\n",
            "Epoch 20/100\n",
            "4735/4735 [==============================] - 2631s 556ms/step - loss: 1.1951 - dense3_loss: 2.1044 - vit__loss: 0.8917 - dense3_accuracy: 0.4915 - vit__accuracy: 0.7685 - average_accuracy: 0.7641 - val_loss: 0.6388 - val_dense3_loss: 1.4768 - val_vit__loss: 0.3591 - val_dense3_accuracy: 0.6165 - val_vit__accuracy: 0.8994 - val_average_accuracy: 0.8917 - lr: 8.1537e-05\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 7.827577896959998e-05.\n",
            "Epoch 21/100\n",
            "4735/4735 [==============================] - 2634s 556ms/step - loss: 1.1846 - dense3_loss: 2.0685 - vit__loss: 0.8897 - dense3_accuracy: 0.4996 - vit__accuracy: 0.7704 - average_accuracy: 0.7682 - val_loss: 0.6223 - val_dense3_loss: 1.4221 - val_vit__loss: 0.3555 - val_dense3_accuracy: 0.6312 - val_vit__accuracy: 0.9001 - val_average_accuracy: 0.8917 - lr: 7.8276e-05\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 7.827577896959998e-05.\n",
            "Epoch 22/100\n",
            "4735/4735 [==============================] - 2633s 556ms/step - loss: 1.1677 - dense3_loss: 2.0328 - vit__loss: 0.8791 - dense3_accuracy: 0.5059 - vit__accuracy: 0.7714 - average_accuracy: 0.7691 - val_loss: 0.6186 - val_dense3_loss: 1.4194 - val_vit__loss: 0.3515 - val_dense3_accuracy: 0.6305 - val_vit__accuracy: 0.9018 - val_average_accuracy: 0.8927 - lr: 7.8276e-05\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 7.827577896959998e-05.\n",
            "Epoch 23/100\n",
            "4735/4735 [==============================] - 2633s 556ms/step - loss: 1.1564 - dense3_loss: 2.0014 - vit__loss: 0.8745 - dense3_accuracy: 0.5138 - vit__accuracy: 0.7727 - average_accuracy: 0.7709 - val_loss: 0.6048 - val_dense3_loss: 1.3724 - val_vit__loss: 0.3488 - val_dense3_accuracy: 0.6444 - val_vit__accuracy: 0.9025 - val_average_accuracy: 0.8941 - lr: 7.8276e-05\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 7.827577896959998e-05.\n",
            "Epoch 24/100\n",
            "4735/4735 [==============================] - 2633s 556ms/step - loss: 1.1453 - dense3_loss: 1.9757 - vit__loss: 0.8684 - dense3_accuracy: 0.5192 - vit__accuracy: 0.7724 - average_accuracy: 0.7706 - val_loss: 0.6007 - val_dense3_loss: 1.3592 - val_vit__loss: 0.3478 - val_dense3_accuracy: 0.6453 - val_vit__accuracy: 0.9028 - val_average_accuracy: 0.8935 - lr: 7.8276e-05\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 7.514474781081598e-05.\n",
            "Epoch 25/100\n",
            "4735/4735 [==============================] - 2628s 555ms/step - loss: 1.1326 - dense3_loss: 1.9327 - vit__loss: 0.8658 - dense3_accuracy: 0.5279 - vit__accuracy: 0.7750 - average_accuracy: 0.7734 - val_loss: 0.5872 - val_dense3_loss: 1.3140 - val_vit__loss: 0.3447 - val_dense3_accuracy: 0.6570 - val_vit__accuracy: 0.9037 - val_average_accuracy: 0.8943 - lr: 7.5145e-05\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 7.514474781081598e-05.\n",
            "Epoch 26/100\n",
            "4735/4735 [==============================] - 2629s 555ms/step - loss: 1.1262 - dense3_loss: 1.9108 - vit__loss: 0.8645 - dense3_accuracy: 0.5348 - vit__accuracy: 0.7760 - average_accuracy: 0.7751 - val_loss: 0.5812 - val_dense3_loss: 1.2921 - val_vit__loss: 0.3440 - val_dense3_accuracy: 0.6603 - val_vit__accuracy: 0.9037 - val_average_accuracy: 0.8952 - lr: 7.5145e-05\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 7.514474781081598e-05.\n",
            "Epoch 27/100\n",
            "4735/4735 [==============================] - 2628s 555ms/step - loss: 1.1106 - dense3_loss: 1.8803 - vit__loss: 0.8539 - dense3_accuracy: 0.5402 - vit__accuracy: 0.7767 - average_accuracy: 0.7769 - val_loss: 0.5800 - val_dense3_loss: 1.2867 - val_vit__loss: 0.3442 - val_dense3_accuracy: 0.6644 - val_vit__accuracy: 0.9042 - val_average_accuracy: 0.8939 - lr: 7.5145e-05\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 7.514474781081598e-05.\n",
            "Epoch 28/100\n",
            "4735/4735 [==============================] - 2628s 555ms/step - loss: 1.1048 - dense3_loss: 1.8565 - vit__loss: 0.8540 - dense3_accuracy: 0.5458 - vit__accuracy: 0.7785 - average_accuracy: 0.7790 - val_loss: 0.5724 - val_dense3_loss: 1.2672 - val_vit__loss: 0.3407 - val_dense3_accuracy: 0.6664 - val_vit__accuracy: 0.9041 - val_average_accuracy: 0.8940 - lr: 7.5145e-05\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 7.213895789838334e-05.\n",
            "Epoch 29/100\n",
            "4735/4735 [==============================] - 2634s 556ms/step - loss: 1.0996 - dense3_loss: 1.8365 - vit__loss: 0.8539 - dense3_accuracy: 0.5513 - vit__accuracy: 0.7770 - average_accuracy: 0.7772 - val_loss: 0.5604 - val_dense3_loss: 1.2280 - val_vit__loss: 0.3378 - val_dense3_accuracy: 0.6765 - val_vit__accuracy: 0.9051 - val_average_accuracy: 0.8943 - lr: 7.2139e-05\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 7.213895789838334e-05.\n",
            "Epoch 30/100\n",
            "4735/4735 [==============================] - 2635s 556ms/step - loss: 1.0799 - dense3_loss: 1.8069 - vit__loss: 0.8375 - dense3_accuracy: 0.5591 - vit__accuracy: 0.7815 - average_accuracy: 0.7840 - val_loss: 0.5594 - val_dense3_loss: 1.2246 - val_vit__loss: 0.3376 - val_dense3_accuracy: 0.6790 - val_vit__accuracy: 0.9056 - val_average_accuracy: 0.8938 - lr: 7.2139e-05\n",
            "\n",
            "Epoch 31: LearningRateScheduler setting learning rate to 7.213895789838334e-05.\n",
            "Epoch 31/100\n",
            "4735/4735 [==============================] - 2634s 556ms/step - loss: 1.0857 - dense3_loss: 1.8010 - vit__loss: 0.8472 - dense3_accuracy: 0.5591 - vit__accuracy: 0.7794 - average_accuracy: 0.7805 - val_loss: 0.5537 - val_dense3_loss: 1.2013 - val_vit__loss: 0.3377 - val_dense3_accuracy: 0.6840 - val_vit__accuracy: 0.9058 - val_average_accuracy: 0.8958 - lr: 7.2139e-05\n",
            "\n",
            "Epoch 32: LearningRateScheduler setting learning rate to 7.213895789838334e-05.\n",
            "Epoch 32/100\n",
            "4735/4735 [==============================] - 2624s 554ms/step - loss: 1.0704 - dense3_loss: 1.7736 - vit__loss: 0.8360 - dense3_accuracy: 0.5627 - vit__accuracy: 0.7818 - average_accuracy: 0.7824 - val_loss: 0.5445 - val_dense3_loss: 1.1726 - val_vit__loss: 0.3350 - val_dense3_accuracy: 0.6906 - val_vit__accuracy: 0.9061 - val_average_accuracy: 0.8964 - lr: 7.2139e-05\n",
            "\n",
            "Epoch 33: LearningRateScheduler setting learning rate to 6.9253399582448e-05.\n",
            "Epoch 33/100\n",
            "3433/4735 [====================>.........] - ETA: 9:47 - loss: 1.0700 - dense3_loss: 1.7605 - vit__loss: 0.8398 - dense3_accuracy: 0.5685 - vit__accuracy: 0.7797 - average_accuracy: 0.7805"
          ]
        }
      ]
    }
  ]
}