{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.13"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "r6NgmaME_1rK",
        "Tzxt5nDla2bc",
        "Bzhcax0yEREe",
        "iD-zp0QPi7es",
        "NoXVD9GWhyoL"
      ],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tzxt5nDla2bc"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo1H0lVFjOQ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8232377c-5816-44cb-e100-83986b64c66c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsqmGk_kaf6s"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.enable_eager_execution()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_ngCHVwZ8qi"
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop, SGD, Adam,Adamax\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.keras.losses import Loss\n",
        "from tensorflow.python.keras.utils import losses_utils\n",
        "from tensorflow.python.keras.utils import tf_utils\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.python.ops import gen_math_ops\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, ZeroPadding2D,Dropout, Dense, Input, concatenate,GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import  AveragePooling2D, Flatten,ZeroPadding2D,AveragePooling1D,Activation,BatchNormalization\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.optimizers import RMSprop, SGD, Adam,Adadelta,Adamax\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau,LearningRateScheduler\n",
        "from keras.models import load_model\n",
        "import math"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cdR2sjRW4kx"
      },
      "source": [
        "# DATA *GENERATOR*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CmI7dgf3Gr0n"
      },
      "outputs": [],
      "source": [
        "#https://github.com/sayakpaul/ViT-jax2tf/blob/main/fine_tune.ipynb\n",
        "BATCH_SIZE = 32\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "num_classes = 102\n",
        "\n",
        "def make_dataset(dataset: tf.data.Dataset, train: bool, image_size: int = 224):\n",
        "    def preprocess(image, label):\n",
        "        # For training, do a random crop and horizontal flip.\n",
        "        if train:\n",
        "            channels = image.shape[-1]\n",
        "            begin, size, _ = tf.image.sample_distorted_bounding_box(\n",
        "                tf.shape(image),\n",
        "                tf.zeros([0, 0, 4], tf.float32),\n",
        "                area_range=(0.05, 1.0),\n",
        "                min_object_covered=0,\n",
        "                use_image_if_no_bounding_boxes=True,\n",
        "            )\n",
        "            image = tf.slice(image, begin, size)\n",
        "\n",
        "            image.set_shape([None, None, channels])\n",
        "            image = tf.image.resize(image, [image_size, image_size])\n",
        "            if tf.random.uniform(shape=[]) > 0.5:\n",
        "                image = tf.image.flip_left_right(image)\n",
        "\n",
        "        else:\n",
        "            image = tf.image.resize(image, [image_size, image_size])\n",
        "\n",
        "        image = (image - 127.5) / 127.5\n",
        "        return image, label\n",
        "\n",
        "    if train:\n",
        "        dataset = dataset.shuffle(BATCH_SIZE * 10)\n",
        "    return dataset.map(preprocess, AUTO).batch(BATCH_SIZE).prefetch(AUTO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-zBXdg8H9me",
        "outputId": "1ab08cd6-1e38-4945-9ff5-fdbfb9a1fc3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 1020\n",
            "Number of validation examples: 6149\n"
          ]
        }
      ],
      "source": [
        "#https://github.com/KlrShaK/Oxford_flowers102-using-Tensorflow/blob/master/Classifier/oxford_flower102.py\n",
        "dataset_name = 'oxford_flowers102'\n",
        "train_dataset = tfds.load(dataset_name, split=tfds.Split.VALIDATION, as_supervised=True)\n",
        "val_dataset = tfds.load(dataset_name, split=tfds.Split.TEST, as_supervised=True)\n",
        "num_train = tf.data.experimental.cardinality(train_dataset)\n",
        "num_val = tf.data.experimental.cardinality(val_dataset)\n",
        "print(f\"Number of training examples: {num_train}\")\n",
        "print(f\"Number of validation examples: {num_val}\")\n",
        "train_dataset = make_dataset(train_dataset, True)\n",
        "val_dataset = make_dataset(val_dataset, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "bIWGAWUhV1QH"
      },
      "outputs": [],
      "source": [
        "b1_vit=\"https://tfhub.dev/sayakpaul/vit_b8_classification/1\"\n",
        "feature_extractor_model = b1_vit\n",
        "feature_extractor_layer = hub.KerasLayer(feature_extractor_model,input_shape=(224,224 ,3),trainable=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD-zp0QPi7es"
      },
      "source": [
        "# Model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l2=tf.keras.regularizers.l2(1e-4)\n",
        "l1l2=regularizers.l1_l2(l1=1e-5, l2=1e-4)\n",
        "l1=tf.keras.regularizers.l1(1e-5)\n",
        "kernel_init = tf.keras.initializers.he_normal()\n",
        "l2=regularizers.l2(0.01)\n",
        "bias_init = tf.keras.initializers.Constant(value=0.1)"
      ],
      "metadata": {
        "id": "rIbTZK8roga7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FYOdngzf2bH"
      },
      "source": [
        "def inception_module1(x,\n",
        "                     filters_3x3,\n",
        "                     filters_3x3_1,\n",
        "                     filters_3x3_2,\n",
        "                     filters_5x5,\n",
        "                     filters_5x5_1,\n",
        "                     filters_5x5_2,\n",
        "                     filters_pool_proj,\n",
        "                     name='inc1'):\n",
        "\n",
        "    conv_3x3 = Conv2D(filters_3x3, (1, 1), padding='same', activation='elu', kernel_initializer=\"he_normal\",trainable=True)(x)\n",
        "    conv_3x3_1 = Conv2D(filters_3x3_1, (3, 3), padding='same', activation='elu', kernel_initializer=\"he_normal\",trainable=True)(conv_3x3)\n",
        "    conv_3x3_2 = Conv2D(filters_3x3_2, (3, 3), padding='same', activation='elu', kernel_initializer=\"he_normal\",kernel_regularizer=l2,trainable=True)(conv_3x3)\n",
        "\n",
        "    conv_5x5 = Conv2D(filters_5x5, (1, 1), padding='same', activation='elu', kernel_initializer=kernel_init,trainable=True)(x)\n",
        "    conv_5x5_1= Conv2D(filters_5x5_1, (5, 5), padding='same', activation='elu', kernel_initializer=\"he_normal\",trainable=True)(conv_5x5)\n",
        "    conv_5x5_2 = Conv2D(filters_5x5_2, (5, 5), padding='same', activation='elu', kernel_initializer=\"he_normal\",kernel_regularizer=l2,trainable=True)(conv_5x5)\n",
        "\n",
        "    pool_proj = MaxPool2D((3, 3), strides=(1, 1), padding='same')(x)\n",
        "    pool_proj = Conv2D(filters_pool_proj, (1, 1), padding='same', activation='elu', kernel_initializer=\"he_normal\")(pool_proj)\n",
        "\n",
        "    output = concatenate([conv_3x3_1, conv_3x3_2, conv_5x5_1,conv_5x5_2, pool_proj], axis=3, name='inc1')\n",
        "\n",
        "    return output"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1meRFxyP7gfa"
      },
      "source": [
        "def inception_module2(x,\n",
        "                     filters_3x3,\n",
        "                     filters_3x3_1,\n",
        "                     filters_3x3_2,\n",
        "                     filters_5x5,\n",
        "                     filters_5x5_1,\n",
        "                     filters_5x5_2,\n",
        "                     filters_pool_proj,\n",
        "                     name='inc21'):\n",
        "\n",
        "    conv_3x3 = Conv2D(filters_3x3, (1, 1), padding='same', activation='elu', kernel_initializer=\"he_normal\",trainable=True)(x)\n",
        "    conv_3x3_1 = Conv2D(filters_3x3_1, (3, 3), padding='same', activation='elu', kernel_initializer=\"he_normal\",trainable=True)(conv_3x3)\n",
        "    conv_3x3_2 = Conv2D(filters_3x3_2, (3, 3), padding='same', activation='elu', kernel_initializer=\"he_normal\",kernel_regularizer=l2,trainable=True)(conv_3x3)\n",
        "\n",
        "    conv_5x5 = Conv2D(filters_5x5, (1, 1), padding='same', activation='elu', kernel_initializer=kernel_init,trainable=True)(x)\n",
        "    conv_5x5_1= Conv2D(filters_5x5_1, (5, 5), padding='same', activation='elu', kernel_initializer=\"he_normal\",trainable=True)(conv_5x5)\n",
        "    conv_5x5_2 = Conv2D(filters_5x5_2, (5, 5), padding='same', activation='elu', kernel_initializer=\"he_normal\",kernel_regularizer=l2,trainable=True)(conv_5x5)\n",
        "\n",
        "    pool_proj = MaxPool2D((3, 3), strides=(1, 1), padding='same')(x)\n",
        "    pool_proj = Conv2D(filters_pool_proj, (1, 1), padding='same', activation='elu', kernel_initializer=\"he_normal\")(pool_proj)\n",
        "\n",
        "    output = concatenate([conv_3x3_1, conv_3x3_2, conv_5x5_1,conv_5x5_2, pool_proj], axis=3, name='inc21')\n",
        "\n",
        "    return output"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwkMRhM0fJ_l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d694c5ec-f5d1-4c84-934f-60cb09f32384"
      },
      "source": [
        "model = Sequential()\n",
        "input_layer = Input(shape=(224,224,3))\n",
        "\n",
        "vit =   feature_extractor_layer(input_layer)\n",
        "x9 = Dense(num_classes, activation='softmax',name='vit_')(vit)\n",
        "\n",
        "x_01=Conv2D(32, (3, 3), padding = 'same', kernel_initializer=\"he_normal\",name='conv2d_10_')(input_layer)\n",
        "x2=Activation('elu')(x_01)\n",
        "x3=BatchNormalization(name='batch_10')(x2)\n",
        "\n",
        "x4=Conv2D(64, (1,1), padding = \"same\", kernel_initializer=\"he_normal\",name='conv1d_1')(x3)\n",
        "\n",
        "x5=Conv2D(64, (3,3), padding = \"same\", kernel_initializer=\"he_normal\",name='conv2d_102')(x4)\n",
        "x6=Activation('elu')(x5)\n",
        "x7=BatchNormalization()(x6)\n",
        "x8=Dropout(0.5)(x7)\n",
        "\n",
        "x9s=Conv2D(64, (3,3), padding = \"same\", kernel_initializer=\"he_normal\",name='conv2d_103')(x8)\n",
        "x0=Activation('elu')(x9s)\n",
        "xy=BatchNormalization()(x0)\n",
        "\n",
        "x0=AveragePooling2D(pool_size=(2, 2))(xy)\n",
        "\n",
        "y1=AveragePooling2D(pool_size=(4, 4))(xy)\n",
        "y1=Dropout(0.2)(y1)\n",
        "\n",
        "x1=x0\n",
        "##############################################\n",
        "i0=Conv2D(64, (1,1), padding = \"same\", kernel_initializer=\"he_normal\")(x1)\n",
        "\n",
        "i1 = inception_module1(i0,\n",
        "                     filters_3x3=128,\n",
        "                     filters_3x3_1=192,\n",
        "                     filters_3x3_2=192,\n",
        "                     filters_5x5=64,\n",
        "                     filters_5x5_1=96,\n",
        "                     filters_5x5_2=96,\n",
        "                     filters_pool_proj=64,\n",
        "                     name='inc1')\n",
        "\n",
        "i2 = inception_module2(i1,\n",
        "                     filters_3x3=128,\n",
        "                     filters_3x3_1=192,\n",
        "                     filters_3x3_2=192,\n",
        "                     filters_5x5=64,\n",
        "                     filters_5x5_1=96,\n",
        "                     filters_5x5_2=96,\n",
        "                     filters_pool_proj=64,\n",
        "                     name='inc2')\n",
        "\n",
        "i4 = AveragePooling2D(pool_size=(6,6), padding='same', strides=None, name='max_pool_inception_01')(i1)\n",
        "##############################################\n",
        "x1z=Conv2D(128, (3, 3), padding = 'same', kernel_initializer=\"he_normal\",name='conv2d_201')(x1)\n",
        "x1x=Activation('elu')(x1z)\n",
        "x1c=BatchNormalization(name='batch_12')(x1x)\n",
        "\n",
        "x1v=Conv2D(128, (1,1), padding = \"same\", kernel_initializer=\"he_normal\",name='conv1d_2')(x1c)\n",
        "\n",
        "x13=ZeroPadding2D((1, 1))(x1v)\n",
        "x12=Conv2D(128, (3, 3), padding = 'same', kernel_initializer=\"he_normal\",name='conv2d_202')(x13)\n",
        "x18=Activation('elu')(x12)\n",
        "x19=BatchNormalization(name='batch_128')(x18)\n",
        "x126=Conv2D(128, (3, 3), padding = 'valid', kernel_initializer=\"he_normal\",name='conv203')(x19)\n",
        "x1e=Activation('elu')(x126)\n",
        "x23r=BatchNormalization()(x1e)\n",
        "x23rr=ZeroPadding2D((2,2))(x23r)\n",
        "x23t=Dropout(0.2)   (x23rr)\n",
        "x23= MaxPooling2D(pool_size=(6,6))(x23t)\n",
        "x21= AveragePooling2D(pool_size=(2, 2))(x23r)\n",
        "\n",
        "x2t=concatenate([y1,x21])\n",
        "\n",
        "x3r=Conv2D(256, (3, 3), padding = 'same', kernel_initializer=\"he_normal\",name='conv2d_3_1')(x2t)\n",
        "x3e=Activation('elu')(x3r)\n",
        "x35w=BatchNormalization()(x3e)\n",
        "x34q=Conv2D(256, (1,1), padding = \"same\", kernel_initializer=\"he_normal\",name='conv1d_301')(x35w)\n",
        "x36j=Conv2D(256, (3, 3), padding = 'same', kernel_initializer=\"he_normal\",name='conv2d_302')(x34q)\n",
        "x36h=Activation('elu')(x36j)\n",
        "x3166g=BatchNormalization()(x36h)\n",
        "x3166f=Dropout(0.2)(x3166g)\n",
        "x316=ZeroPadding2D((1,1))(x3166f)\n",
        "x33= MaxPooling2D(pool_size=(3,3))(x316)\n",
        "x3_1=AveragePooling2D(pool_size=(2, 2)) (x3166f)\n",
        "x41=ZeroPadding2D((1, 1))(x3_1)\n",
        "x42=Conv2D(512, (3, 3), padding = 'same', kernel_initializer=\"he_normal\",name='conv2d_401_')(x41)\n",
        "x43=Activation('elu')(x42)\n",
        "x44=BatchNormalization(name='batch521')(x43)\n",
        "x65=Conv2D(512, (1,1), padding = \"same\", kernel_initializer=\"he_normal\",name='conv1d_4_')(x44)\n",
        "x66=Conv2D(512, (3, 3), padding = 'valid', kernel_initializer=\"he_normal\",name='conv2d_402')(x65)\n",
        "x67=Activation('elu')(x66)\n",
        "x68=BatchNormalization()(x67)\n",
        "x7 =AveragePooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(x68)\n",
        "dc_1_0 = Conv2DTranspose(512, (6,6), activation='elu',strides=1,\n",
        "      padding = 'valid',kernel_initializer='he_normal', name='deconv_01_')(x7)\n",
        "dc_1_0=concatenate([i4,dc_1_0,x23,x33])\n",
        "dc_1 = UpSampling2D((2,2))(dc_1_0)\n",
        "x7_0 = Flatten()(dc_1_0)\n",
        "x7_1 = Dense(256, activation='elu',name='dense1')(x7_0)\n",
        "x7_22 =BatchNormalization()(x7_1)\n",
        "x7_2 = Dropout(0.1)(x7_22)\n",
        "x7_4 = Dense(128, activation='elu',name='dense2')(x7_2)\n",
        "x7_51 =BatchNormalization()(x7_4)\n",
        "x7_5 = Dropout(0.1)(x7_51)\n",
        "x8w = Dense(num_classes, activation='softmax',name='dense')(x7_5)\n",
        "avr= tf.keras.layers.Average(name='average')([x8w,x9])\n",
        "#########################\n",
        "model = Model(input_layer, [x8w,x9,avr], name='Combination_Model_01')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer HeNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XawOcnfi7et"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tensorboard callback (functionized because need to create a new one for each model)\n",
        "import datetime\n",
        "def create_tensorboard_callback(dir_name, experiment_name):\n",
        "  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "      log_dir=log_dir\n",
        "  )\n",
        "  print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
        "  return tensorboard_callback\n",
        "\n",
        "#############################\n",
        "\n",
        "  # Setup EarlyStopping callback to stop training if model's val_loss doesn't improve for 3 epochs\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", # watch the val loss metric\n",
        "                                                  patience=10) # if val loss decreases for 3 epochs in a row, stop training\n",
        "##############################\n",
        "# Creating learning rate reduction callback\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
        "                                                 factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x)\n",
        "                                                 patience=2,\n",
        "                                                 verbose=1, # print out when learning rate goes down\n",
        "                                                 min_lr=1e-7)\n",
        "###############################\n",
        "def decay(epochs, steps=100):\n",
        "    initial_lrate = 0.0001\n",
        "    drop = 0.96\n",
        "    epochs_drop = 8\n",
        "    lrate = initial_lrate * math.pow(drop, math.floor((1+(2*epochs)/epochs_drop)))\n",
        "    return lrate\n",
        "################################\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"/flower_1.h5\",\n",
        "                             #save the best model while all epovhs of training\n",
        "                             monitor=\"val_loss\",\n",
        "                             save_best_only=True, # only save the best weights\n",
        "                             save_weights_only=True, # only save model weights (not whole model)\n",
        "                             mode=\"min\",\n",
        "                             verbose=1)\n",
        "\n",
        "#################################\n",
        "lr_sc = LearningRateScheduler(decay,verbose=1)\n",
        "mycallback = [early_stopping,lr_sc,reduce_lr,checkpoint,\n",
        "              create_tensorboard_callback(dir_name=\"tensorflow_hub\",\n",
        "              experiment_name=\"ENSEMBLE MODEL CIFAR10_Exp2\")]"
      ],
      "metadata": {
        "id": "ccNIYxWEqR7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model('/content/drive/MyDrive/oxford-flower-weights/ensemble_stanfordflower_model3_5')"
      ],
      "metadata": {
        "id": "aPhze9n5fuLV"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_train_samples = 1020\n",
        "nb_validation_samples = 6149\n",
        "\n",
        "lossWeights = {'dense':0.8,'vit_':0.2}\n",
        "loss = keras.losses.CategoricalCrossentropy().SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "optimizer =Adamax(learning_rate=0.0001, clipnorm=1.0)#optimizer =ad\n",
        "\n",
        "model.compile( optimizer=optimizer, metrics=[\"accuracy\"],loss={'dense':loss,'vit_':loss},loss_weights=lossWeights)\n",
        "\n",
        "history = model.fit(train_dataset\n",
        "                    ,validation_data=val_dataset\n",
        "                    ,epochs=20\n",
        "                    ,callbacks=[mycallback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5c0f22f-b8bc-428c-eb5f-b433528b7d98",
        "id": "X5VXxAzTqENe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 9.6e-05.\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 269s 8s/step - loss: 0.7087 - dense_loss: 2.4966 - vit__loss: 0.1035 - dense_accuracy: 0.3451 - vit__accuracy: 0.9755 - average_accuracy: 0.9765 - val_loss: 0.7929 - val_dense_loss: 2.9918 - val_vit__loss: 0.0509 - val_dense_accuracy: 0.2760 - val_vit__accuracy: 0.9927 - val_average_accuracy: 0.9915 - lr: 9.6000e-05\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 9.6e-05.\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 243s 8s/step - loss: 0.6509 - dense_loss: 2.4294 - vit__loss: 0.0490 - dense_accuracy: 0.3569 - vit__accuracy: 0.9863 - average_accuracy: 0.9873 - val_loss: 0.7927 - val_dense_loss: 2.9906 - val_vit__loss: 0.0510 - val_dense_accuracy: 0.2771 - val_vit__accuracy: 0.9925 - val_average_accuracy: 0.9914 - lr: 9.6000e-05\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 9.6e-05.\n",
            "Epoch 3/20\n",
            "31/32 [============================>.] - ETA: 1s - loss: 0.7082 - dense_loss: 2.5038 - vit__loss: 0.1008 - dense_accuracy: 0.3528 - vit__accuracy: 0.9819 - average_accuracy: 0.9829"
          ]
        }
      ]
    }
  ]
}