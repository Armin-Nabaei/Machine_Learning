{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEc8zjQcgMUf"
      },
      "source": [
        "# Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xWd_QYWHwdZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "from ast import Import\n",
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers,Input,Model\n",
        "from tensorflow.keras.datasets import mnist,cifar10\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D,GlobalMaxPooling2D,AveragePooling2D,UpSampling2D,Conv2DTranspose,GlobalAveragePooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Activation, Flatten, Dropout, Dense,ZeroPadding2D,Con\n",
        "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "tf.config.run_functions_eagerly(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KswyYbbgRTS"
      },
      "source": [
        "#  Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPRr_U6mqJk-"
      },
      "outputs": [],
      "source": [
        "# Prepare the training dataset.\n",
        "batch_size = 64\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Reserve 10,000 samples for validation.\n",
        "x_val = x_train[-10000:]\n",
        "y_val = y_train[-10000:]\n",
        "x_train = x_train[:-10000]\n",
        "y_train = y_train[:-10000]\n",
        "\n",
        "# Prepare the training dataset.\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=64).batch(batch_size)\n",
        "\n",
        "# Prepare the validation dataset.\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "val_dataset = val_dataset.batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M99J1nhygWqE"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPmNJIY7XBXD"
      },
      "outputs": [],
      "source": [
        "\n",
        "input = Input(shape=(32,32,3))\n",
        "\n",
        "x=Conv2D(192, (3, 3), padding = 'same', kernel_initializer=\"he_normal\")(input)\n",
        "x=Activation('elu')(x)\n",
        "x=BatchNormalization()(x)\n",
        "x=Conv2D(192, (3, 3), padding = 'same', kernel_initializer=\"he_normal\")(x)\n",
        "x=Activation('elu')(x)\n",
        "x=BatchNormalization()(x)\n",
        "x=Conv2D(192, (3, 3), padding = 'same', kernel_initializer=\"he_normal\")(x)\n",
        "x=Dropout(0.5)(x)\n",
        "\n",
        "x=Conv2D(192, (3, 3), padding = 'same', kernel_initializer=\"he_normal\")(x)\n",
        "x=Activation('elu')(x)\n",
        "x=BatchNormalization()(x)\n",
        "x=Conv2D(192, (3, 3), padding = 'same', kernel_initializer=\"he_normal\")(x)\n",
        "x=Activation('elu')(x)\n",
        "x=BatchNormalization()(x)\n",
        "x=Conv2D(192, (3, 3), padding = 'same', kernel_initializer=\"he_normal\")(x)\n",
        "x=Dropout(0.5)(x)\n",
        "\n",
        "x=Conv2D(192, (3, 3), padding = 'same', kernel_initializer=\"he_normal\")(x)\n",
        "x=Activation('elu')(x)\n",
        "\n",
        "x=Conv2D(192, (1,1), padding = 'same', kernel_initializer=\"he_normal\")(x)\n",
        "x=Activation('elu')(x)\n",
        "\n",
        "x=Conv2D(10, (1,1), padding = 'same', kernel_initializer=\"he_normal\")(x)\n",
        "\n",
        "\n",
        "x =GlobalAveragePooling2D()(x)\n",
        "output=Activation('softmax')(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h22lce5qgf33"
      },
      "source": [
        "# Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npghQwnxM8Ai"
      },
      "outputs": [],
      "source": [
        "save_callback = keras.callbacks.ModelCheckpoint(\n",
        "    'chekpoint/',\n",
        "    save_weights_only=True,\n",
        "    monitor='accuracy',\n",
        "    save_best_only=False,\n",
        ")\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "  if epoch < 2:\n",
        "    return lr\n",
        "  else:\n",
        "     return lr*0.99\n",
        "\n",
        "lr_Sccheduler = keras.callbacks.LearningRateScheduler (scheduler,verbose=1)\n",
        "\n",
        "\n",
        "class CustomCallback (keras.callbacks.Callback):\n",
        "  def on_epoch_end (self, epoch, logs=None):\n",
        "    #print(logs.keys())\n",
        "    if logs.get(\"accuracy\") > 0.99:\n",
        "      print(\"Accuracy over 90%, qutting training\")\n",
        "      #self.model.stop_training = True\n",
        "\n",
        "mycallback = CustomCallback()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cfyEMSLrDO-"
      },
      "outputs": [],
      "source": [
        "# Instantiate an optimizer to train the model.\n",
        "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-3)\n",
        "# Instantiate a loss function.\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "# Prepare the metrics.\n",
        "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "model = keras.Model(inputs=input, outputs=output)\n",
        "\n",
        "#import datetime\n",
        "#https://www.tensorflow.org/tensorboard/get_started\n",
        "train_writer = tf.summary.create_file_writer(\"logs/train/\")\n",
        "test_writer = tf.summary.create_file_writer(\"logs/test/\")\n",
        "train_step = test_step = 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proposed Scale Up Gradient Function"
      ],
      "metadata": {
        "id": "StoIi_GAbpaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def gradi(gradients):\n",
        "\n",
        "        for i in range(len(gradients)):\n",
        "\n",
        "            mean=0\n",
        "            a=0\n",
        "            b=0\n",
        "            c=0\n",
        "         # CHOOOSIG LAYER OE KEREL TO MIDIFY\n",
        "            if gradients[i].shape == [3,3,3,32] :\n",
        "               if i==0:\n",
        "                  gradient_new = gradients\n",
        "                  gradient_trasposed=gradients\n",
        "                  gradient_new1=gradients\n",
        "                  boolean_mask=gradients\n",
        "                  gradient_new2=gradients\n",
        "                  mean = tf.math.reduce_mean(gradients[i], keepdims=True)\n",
        "                  maskv=[]\n",
        "                  for e in range (3):\n",
        "                        for r in range (32):\n",
        "                            for h in range (3):\n",
        "                                 for d in range (3):\n",
        "                                     mean_kernel = tf.math.reduce_mean((tf.Variable(gradients [0][e,:,:,r])), keepdims=False)\n",
        "                                     std_kernel =tf.math.reduce_std( (tf.Variable(gradients [0][e,:,:,r])), keepdims=False)\n",
        "                                     t = tf.Variable(gradients [0][e,h,d,r])\n",
        "                                     if t>1 :\n",
        "                                        gaus= 2 * tf.math.exp(- (tf.math.pow((t-mean_kernel),2))/2*tf.math.square(std_kernel))\n",
        "                                     else:\n",
        "                                        gaus = 0\n",
        "\n",
        "                                     tf.compat.v1.assign((tf.Variable(gradient_new [0][e,h,d,r])),gaus)\n",
        "\n",
        "                  gradients[0] = gradient_new [0]\n",
        "            else:\n",
        "                 gradients [i] = gradients [i]\n",
        "        return gradients"
      ],
      "metadata": {
        "id": "yW7HC3zyp3bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Test Functions"
      ],
      "metadata": {
        "id": "PkDTY4A8cSUH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CB_pvVYhpnXU"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x, training=True)\n",
        "        loss_value = loss_fn(y, logits)\n",
        "        tf.executing_eagerly()\n",
        "    gradients = tape.gradient(loss_value, model.trainable_weights)\n",
        "    #Call Proposed Function\n",
        "    grads = gradi (gradients)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    train_acc_metric.update_state(y, logits)\n",
        "    return loss_value\n",
        "\n",
        "@tf.function\n",
        "def test_step(x, y):\n",
        "    val_logits = model(x, training=False)\n",
        "    val_loss_value = loss_fn(y, val_logits)\n",
        "    val_acc = val_acc_metric.update_state(y, val_logits)\n",
        "    return [val_acc,val_loss_value]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fitting"
      ],
      "metadata": {
        "id": "T3tzPICdcrmu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hT8mpkAgptqQ",
        "outputId": "b7e52ad9-3656-4aeb-97ed-8ec6c15fef0c"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "Training acc over epoch: 0.4205\n",
            "Training loss over epoch: 1.4679\n",
            "Validation acc: 0.4807\n",
            "Validation Loss : 2.2742\n",
            "Time taken: 107.83s\n",
            "\n",
            "Start of epoch 1\n",
            "Training acc over epoch: 0.5917\n",
            "Training loss over epoch: 1.0891\n",
            "Validation acc: 0.6286\n",
            "Validation Loss : 1.7976\n",
            "Time taken: 103.45s\n",
            "\n",
            "Start of epoch 2\n",
            "Training acc over epoch: 0.6638\n",
            "Training loss over epoch: 0.8896\n",
            "Validation acc: 0.6473\n",
            "Validation Loss : 1.9192\n",
            "Time taken: 106.22s\n",
            "\n",
            "Start of epoch 3\n",
            "Training acc over epoch: 0.7104\n",
            "Training loss over epoch: 0.7118\n",
            "Validation acc: 0.6967\n",
            "Validation Loss : 1.4390\n",
            "Time taken: 103.27s\n",
            "\n",
            "Start of epoch 4\n",
            "Training acc over epoch: 0.7489\n",
            "Training loss over epoch: 0.7111\n",
            "Validation acc: 0.6985\n",
            "Validation Loss : 1.3461\n",
            "Time taken: 102.62s\n",
            "\n",
            "Start of epoch 5\n",
            "Training acc over epoch: 0.7742\n",
            "Training loss over epoch: 0.5948\n",
            "Validation acc: 0.7164\n",
            "Validation Loss : 0.9825\n",
            "Time taken: 102.69s\n",
            "\n",
            "Start of epoch 6\n",
            "Training acc over epoch: 0.7998\n",
            "Training loss over epoch: 0.4461\n",
            "Validation acc: 0.7111\n",
            "Validation Loss : 1.3172\n",
            "Time taken: 149.39s\n",
            "\n",
            "Start of epoch 7\n",
            "Training acc over epoch: 0.8197\n",
            "Training loss over epoch: 0.4165\n",
            "Validation acc: 0.7291\n",
            "Validation Loss : 1.0976\n",
            "Time taken: 103.16s\n",
            "\n",
            "Start of epoch 8\n",
            "Training acc over epoch: 0.8374\n",
            "Training loss over epoch: 0.3223\n",
            "Validation acc: 0.6910\n",
            "Validation Loss : 1.2177\n",
            "Time taken: 104.87s\n",
            "\n",
            "Start of epoch 9\n",
            "Training acc over epoch: 0.8499\n",
            "Training loss over epoch: 0.3916\n",
            "Validation acc: 0.7731\n",
            "Validation Loss : 0.8959\n",
            "Time taken: 102.38s\n",
            "\n",
            "Start of epoch 10\n",
            "Training acc over epoch: 0.8628\n",
            "Training loss over epoch: 0.2986\n",
            "Validation acc: 0.7980\n",
            "Validation Loss : 0.8140\n",
            "Time taken: 149.39s\n",
            "\n",
            "Start of epoch 11\n",
            "Training acc over epoch: 0.8740\n",
            "Training loss over epoch: 0.2878\n",
            "Validation acc: 0.7940\n",
            "Validation Loss : 0.7773\n",
            "Time taken: 105.80s\n",
            "\n",
            "Start of epoch 12\n",
            "Training acc over epoch: 0.8806\n",
            "Training loss over epoch: 0.2766\n",
            "Validation acc: 0.7940\n",
            "Validation Loss : 0.6786\n",
            "Time taken: 102.14s\n",
            "\n",
            "Start of epoch 13\n",
            "Training acc over epoch: 0.8906\n",
            "Training loss over epoch: 0.2731\n",
            "Validation acc: 0.8134\n",
            "Validation Loss : 0.5932\n",
            "Time taken: 149.38s\n",
            "\n",
            "Start of epoch 14\n",
            "Training acc over epoch: 0.8998\n",
            "Training loss over epoch: 0.2278\n",
            "Validation acc: 0.8094\n",
            "Validation Loss : 0.6029\n",
            "Time taken: 102.13s\n",
            "\n",
            "Start of epoch 15\n",
            "Training acc over epoch: 0.9086\n",
            "Training loss over epoch: 0.1794\n",
            "Validation acc: 0.8008\n",
            "Validation Loss : 0.7313\n",
            "Time taken: 102.25s\n",
            "\n",
            "Start of epoch 16\n",
            "Training acc over epoch: 0.9132\n",
            "Training loss over epoch: 0.1601\n",
            "Validation acc: 0.8295\n",
            "Validation Loss : 0.7393\n",
            "Time taken: 101.91s\n",
            "\n",
            "Start of epoch 17\n",
            "Training acc over epoch: 0.9234\n",
            "Training loss over epoch: 0.1770\n",
            "Validation acc: 0.8110\n",
            "Validation Loss : 0.5282\n",
            "Time taken: 101.52s\n",
            "\n",
            "Start of epoch 18\n",
            "Training acc over epoch: 0.9262\n",
            "Training loss over epoch: 0.1125\n",
            "Validation acc: 0.7756\n",
            "Validation Loss : 0.9046\n",
            "Time taken: 101.74s\n",
            "\n",
            "Start of epoch 19\n",
            "Training acc over epoch: 0.9327\n",
            "Training loss over epoch: 0.2115\n",
            "Validation acc: 0.8151\n",
            "Validation Loss : 0.7887\n",
            "Time taken: 102.02s\n",
            "\n",
            "Start of epoch 20\n",
            "Training acc over epoch: 0.9352\n",
            "Training loss over epoch: 0.1226\n",
            "Validation acc: 0.7599\n",
            "Validation Loss : 0.9284\n",
            "Time taken: 101.37s\n",
            "\n",
            "Start of epoch 21\n",
            "Training acc over epoch: 0.9405\n",
            "Training loss over epoch: 0.1754\n",
            "Validation acc: 0.7998\n",
            "Validation Loss : 1.1586\n",
            "Time taken: 101.14s\n",
            "\n",
            "Start of epoch 22\n",
            "Training acc over epoch: 0.9420\n",
            "Training loss over epoch: 0.0771\n",
            "Validation acc: 0.8131\n",
            "Validation Loss : 0.5092\n",
            "Time taken: 101.32s\n",
            "\n",
            "Start of epoch 23\n",
            "Training acc over epoch: 0.9467\n",
            "Training loss over epoch: 0.0920\n",
            "Validation acc: 0.8186\n",
            "Validation Loss : 0.8498\n",
            "Time taken: 104.32s\n",
            "\n",
            "Start of epoch 24\n",
            "Training acc over epoch: 0.9503\n",
            "Training loss over epoch: 0.1507\n",
            "Validation acc: 0.7976\n",
            "Validation Loss : 0.5818\n",
            "Time taken: 101.39s\n",
            "\n",
            "Start of epoch 25\n",
            "Training acc over epoch: 0.9522\n",
            "Training loss over epoch: 0.1248\n",
            "Validation acc: 0.8108\n",
            "Validation Loss : 1.2378\n",
            "Time taken: 101.00s\n",
            "\n",
            "Start of epoch 26\n",
            "Training acc over epoch: 0.9544\n",
            "Training loss over epoch: 0.0851\n",
            "Validation acc: 0.7941\n",
            "Validation Loss : 1.0194\n",
            "Time taken: 101.33s\n",
            "\n",
            "Start of epoch 27\n",
            "Training acc over epoch: 0.9546\n",
            "Training loss over epoch: 0.0384\n",
            "Validation acc: 0.8403\n",
            "Validation Loss : 0.7581\n",
            "Time taken: 101.76s\n",
            "\n",
            "Start of epoch 28\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "gradients =[]\n",
        "epochs = 31\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        loss_value = train_step(x_batch_train, y_batch_train)\n",
        "\n",
        "    # Display metrics at the end of each epoch.\n",
        "    train_acc = train_acc_metric.result()\n",
        "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
        "    print(\"Training loss over epoch: %.4f\"% (float(loss_value)))\n",
        "\n",
        "    # Tensorboard Writing Log\n",
        "    with train_writer.as_default():\n",
        "      tf.summary.scalar(\"Adam Kernelized_loss\",loss_value,step=epoch)\n",
        "      tf.summary.scalar (\"Adam Kernelized_Accuracy\",train_acc, step=epoch)\n",
        "\n",
        "    # Reset training metrics at the end of each epoch\n",
        "    train_acc_metric.reset_states()\n",
        "\n",
        "    # Run a validation loop at the end of each epoch.\n",
        "    for x_batch_val, y_batch_val in val_dataset:\n",
        "        val_acc,val_loss_value = test_step(x_batch_val, y_batch_val)\n",
        "\n",
        "    val_acc = val_acc_metric.result()\n",
        "    val_loss_value = val_loss_value\n",
        "\n",
        "    # Tensorboard Writing Log\n",
        "    with test_writer.as_default():\n",
        "      tf.summary.scalar(\"Adam Kernelized_loss\",val_loss_value,step=epoch)\n",
        "      tf.summary.scalar (\"Adam Kernelized_Accuracy\",val_acc, step=epoch)\n",
        "\n",
        "\n",
        "    val_acc_metric.reset_states()\n",
        "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
        "    print(\"Validation Loss : %.4f\" % (float(val_loss_value),))\n",
        "    print(\"Time taken: %.2fs\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF9pY9NkGoNP"
      },
      "source": [
        "# Tensorbord Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edz5iYVIGnXm",
        "outputId": "613e4d10-9742-4872-deca-46f3ac7696a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "***** TensorBoard Uploader *****\n",
            "\n",
            "This will upload your TensorBoard logs to https://tensorboard.dev/ from\n",
            "the following directory:\n",
            "\n",
            "./logs\n",
            "\n",
            "This TensorBoard will be visible to everyone. Do not upload sensitive\n",
            "data.\n",
            "\n",
            "Your use of this service is subject to Google's Terms of Service\n",
            "<https://policies.google.com/terms> and Privacy Policy\n",
            "<https://policies.google.com/privacy>, and TensorBoard.dev's Terms of Service\n",
            "<https://tensorboard.dev/policy/terms/>.\n",
            "\n",
            "This notice will not be shown again while you are logged into the uploader.\n",
            "To log out, run `tensorboard dev auth revoke`.\n",
            "\n",
            "Continue? (yes/NO) yes\n",
            "\n",
            "To sign in with the TensorBoard uploader:\n",
            "\n",
            "1. On your computer or phone, visit:\n",
            "\n",
            "   https://www.google.com/device\n",
            "\n",
            "2. Sign in with your Google account, then enter:\n",
            "\n",
            "   GSYY-VJDS\n",
            "\n",
            "\n",
            "\n",
            "New experiment created. View your TensorBoard at: https://tensorboard.dev/experiment/30APEr2fRQaqhK9koh125g/\n",
            "\n",
            "\u001b[1m[2023-05-23T06:49:05]\u001b[0m Started scanning logdir.\n",
            "\u001b[1m[2023-05-23T06:49:06]\u001b[0m Total uploaded: 250 scalars, 0 tensors, 0 binary objects\n",
            "\u001b[1m[2023-05-23T06:49:06]\u001b[0m Done scanning logdir.\n",
            "\n",
            "\n",
            "Done. View your TensorBoard at https://tensorboard.dev/experiment/30APEr2fRQaqhK9koh125g/\n"
          ]
        }
      ],
      "source": [
        "!tensorboard dev upload --logdir ./\"logs\" \\\n",
        "--name \"My Oprimizer vs Typicall Optimizer\" \\\n",
        "--description \" Kernelized Adam Optimizer vs Standard Adam Optimizers\" \\\n",
        "--one_shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mU81cCC-mPo",
        "outputId": "cf05d470-bb7b-4ac1-c334-ebb1dc0e5671"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy over Test Set: 0.9991666674613953\n"
          ]
        }
      ],
      "source": [
        "#Test Loop\n",
        "\n",
        "for batch_idx, (x_batch, y_batch) in enumerate (ds_test):\n",
        "  y_pred = model( x_batch)\n",
        "  acc_metric.update_state (y_batch,y_pred)\n",
        "\n",
        "train_acc = acc_metric.result()\n",
        "print(f\"Accuracy over Test Set: {train_acc}\")\n",
        "acc_metric.reset_states()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "VEc8zjQcgMUf",
        "aG6iEwzTgZmh",
        "rtfKZW6bgbvS",
        "UWaEXMGvxfRy",
        "QBBqsD3rxijt",
        "FelHtN25RAEg",
        "h22lce5qgf33",
        "uhF8x0obFJBj"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
